{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0408\n",
      "Sequence: Hello I'm a virtual model.\n",
      "Token String: virtual\n",
      "---\n",
      "Score: 0.0200\n",
      "Sequence: Hello I'm a big model.\n",
      "Token String: big\n",
      "---\n",
      "Score: 0.0187\n",
      "Sequence: Hello I'm a Hello model.\n",
      "Token String: Hello\n",
      "---\n",
      "Score: 0.0174\n",
      "Sequence: Hello I'm a model model.\n",
      "Token String: model\n",
      "---\n",
      "Score: 0.0142\n",
      "Sequence: Hello I'm a perfect model.\n",
      "Token String: perfect\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Crear el pipeline de unmasked\n",
    "unmasker = pipeline('fill-mask', model='distilbert-base-multilingual-cased')\n",
    "\n",
    "#Usamos el pipeline para completar la máscara\n",
    "resultados = unmasker(\"Hello I'm a [MASK] model.\")\n",
    "\n",
    "for resultado in resultados:\n",
    "    print(f\"Score: {resultado['score']:.4f}\")\n",
    "    print(f\"Sequence: {resultado['sequence']}\")\n",
    "    print(f\"Token String: {resultado['token_str']}\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBAMOS EN ESPAÑOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0966\n",
      "Sequence: Hola, soy un mundo virtual\n",
      "Token String: mundo\n",
      "---\n",
      "Score: 0.0425\n",
      "Sequence: Hola, soy un universo virtual\n",
      "Token String: universo\n",
      "---\n",
      "Score: 0.0411\n",
      "Sequence: Hola, soy un espacio virtual\n",
      "Token String: espacio\n",
      "---\n",
      "Score: 0.0353\n",
      "Sequence: Hola, soy un personaje virtual\n",
      "Token String: personaje\n",
      "---\n",
      "Score: 0.0286\n",
      "Sequence: Hola, soy un hogar virtual\n",
      "Token String: hogar\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#AHORA PROBAMOS EN ESPAÑOL PARA VER LOS RESULTADOS\n",
    "resultados2= unmasker(\"Hola, soy un [MASK] virtual\")\n",
    "\n",
    "for resultado in resultados2:\n",
    "    print(f\"Score: {resultado['score']:.4f}\")\n",
    "    print(f\"Sequence: {resultado['sequence']}\")\n",
    "    print(f\"Token String: {resultado['token_str']}\")\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AHORA HAREMOS EL PIPELINE PERO DE CLASIFICACIÓN DE TEXTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Cursos\\Curso BOOTCAMP IA GENERATIVA-SOLUCIONES CON PYTHON\\UNIDAD 4\\Tarea4_Modulo_Embeddings\\myvenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: Spiderman\n",
      "Score: 0.9504584074020386\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('text-classification', model='distilbert-base-multilingual-cased')\n",
    "resultados = classifier(\"This is a great Product\")\n",
    "\n",
    "label_map = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "\n",
    "for resultado in resultados:\n",
    "    label = label_map.get(resultado['label'], 'UNKNOWN')\n",
    "    print(f\"Label: {label}, Score: {resultado['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE PUEDE UTILIZAR DISTILLBERT PARA REALIZAR MÁS TAREAS, PERO ESTE MODELO HA SIDO ENTRENADO PARA REALIZAR LA ACTIVIDAD DE MASK. ES POR ELLO QUE NO PUEDO REALIZAR LA CLASIFICACIÓN. VOY A ENTRENARLO CON FINE-TUNING PARA QUE APRENDA A CLASIFICAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos la data que nos brindó el profe \n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "urllib.request.urlretrieve(\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \"aclImdb_v1.tar.gz\")\n",
    "with tarfile.open('aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 171/171 [36:20<00:00, 12.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2180.003, 'train_samples_per_second': 1.239, 'train_steps_per_second': 0.078, 'train_loss': 0.6703060217070997, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=171, training_loss=0.6703060217070997, metrics={'train_runtime': 2180.003, 'train_samples_per_second': 1.239, 'train_steps_per_second': 0.078, 'total_flos': 357661976371200.0, 'train_loss': 0.6703060217070997, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-multilingual-cased\", num_labels=2)\n",
    "\n",
    "# Texto -> [BERT] -> [Clasificación] -> [Positivo, Negativo] [0.5, 0.5] [0, 1]\n",
    "# 1. Inicialización al azar de los pesos del modelo\n",
    "# 2. Feed Forward\n",
    "# 3. Cálculo de la pérdida\n",
    "# 3. Retropropagación\n",
    "# 4. Actualización \n",
    "\n",
    "#TODO: Entrenar el modelo\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora evaluaremos su performance con los datos test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [04:39<00:00, 17.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "#TODO: Cargar los datos de prueba\n",
    "test_dir_pos = 'aclImdb/test/pos/'\n",
    "test_dir_neg = 'aclImdb/test/neg/'\n",
    "\n",
    "test_encodings_pos = load_data(test_dir_pos)\n",
    "test_encodings_neg = load_data(test_dir_neg)\n",
    "\n",
    "test_labels_pos = [1] * len(test_encodings_pos['input_ids'])\n",
    "test_labels_neg = [0] * len(test_encodings_neg['input_ids'])\n",
    "\n",
    "test_encodings = {key: test_encodings_pos[key] + test_encodings_neg[key] for key in test_encodings_pos.keys()}\n",
    "test_labels = test_labels_pos + test_labels_neg\n",
    "\n",
    "#TODO: Crear el dataset y el data loader de prueba\n",
    "test_dataset= TextDataset(test_encodings, test_labels)\n",
    "test_loader= DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "#TODO: Realizar las predicciones. Guardar los logits en preds.\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "\n",
    "#TODO: Calcular el accuracy\n",
    "accuracy = (preds == test_labels).astype(float).mean()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GUARDAMOS EL TOKENIZER DENTRO DE LA CARPETA DEL MODELO PARA PODER UTILIZARLO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/checkpoint-171\\\\tokenizer_config.json',\n",
       " './results/checkpoint-171\\\\special_tokens_map.json',\n",
       " './results/checkpoint-171\\\\vocab.txt',\n",
       " './results/checkpoint-171\\\\added_tokens.json',\n",
       " './results/checkpoint-171\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./results/checkpoint-171')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AHORA PROBAMOS EL NUEVO MODELO ENTRENADO, NOS DAREMOS CUENTA QUE AHORA SI PREDICE CORRECTAMENTE EN LA MAYORÍA DE LOS CASOS, EL ÚLTIMO TEXTO QUE LE PASÉ ES DEL TEST Y LO PREDIJO CORRECTAMENTE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: \"This model is a bit slow and wastes my time\" \n",
      "Predicción: El modelo predice que el texto es negativo.\n",
      "\n",
      "Texto: \"I am very happy with the results obtained\" \n",
      "Predicción: El modelo predice que el texto es positivo.\n",
      "\n",
      "Texto: \"This product is a waste of time and money.\" \n",
      "Predicción: El modelo predice que el texto es positivo.\n",
      "\n",
      "Texto: \"I loved the movie, it was really good.\" \n",
      "Predicción: El modelo predice que el texto es positivo.\n",
      "\n",
      "Texto: \"Customer service was very bad, would not recommend.Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\" \n",
      "Predicción: El modelo predice que el texto es negativo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Ruta al checkpoint guardado\n",
    "checkpoint_path = \"./results/checkpoint-171\"\n",
    "\n",
    "# Cargar el tokenizer y el modelo desde el checkpoint guardado\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Asegúrate de que el modelo esté en modo de evaluación\n",
    "model.eval()\n",
    "\n",
    "def predict_texts(texts, model, tokenizer):\n",
    "    # Preprocesar y tokenizar el texto de entrada\n",
    "    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # Realizar la inferencia\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Obtener la predicción (0 o 1) a partir de los logits\n",
    "    predicted_classes = torch.argmax(logits, dim=-1).tolist()\n",
    "\n",
    "    return predicted_classes\n",
    "\n",
    "# Ejemplo de texto para clasificar\n",
    "nuevos_textos = [\n",
    "    \"This model is a bit slow and wastes my time\",\n",
    "    \"I am very happy with the results obtained\",\n",
    "    \"This product is a waste of time and money.\",\n",
    "    \"I loved the movie, it was really good.\",\n",
    "    \"Customer service was very bad, would not recommend.\"\n",
    "    \"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\"\n",
    "]\n",
    "# Realizar la predicción\n",
    "\n",
    "predicciones = predict_texts(nuevos_textos, model, tokenizer)\n",
    "\n",
    "# Imprimir los resultados\n",
    "for texto, prediccion in zip(nuevos_textos, predicciones):\n",
    "    resultado = \"positivo\" if prediccion == 1 else \"negativo\"\n",
    "    print(f\"Texto: \\\"{texto}\\\" \\nPredicción: El modelo predice que el texto es {resultado}.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AHORA HAREMOS LA GENERACIÓN DE EMBEDDINGS SIMPLE PARA FINALIZAR Y SELECCIONAMOS EL TOKEN CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.63572857e-03 -6.29550219e-03  9.89595950e-02 -1.69998318e-01\n",
      "   9.80851293e-01 -2.77944356e-01 -4.12420779e-01  4.92719620e-01\n",
      "  -1.42921969e-01  2.55764574e-01  2.30696067e-01  9.23453122e-02\n",
      "  -7.97102451e-02 -2.52078295e-01 -3.27376053e-02 -3.92252713e-01\n",
      "  -2.02432007e-01  7.36566365e-01  9.16739777e-02  4.39471364e-01\n",
      "   4.69357252e-01 -9.30169076e-02  1.27542749e-01  1.21282354e-01\n",
      "   1.46919966e-01 -3.04094374e-01  7.71197379e-02 -3.50230336e-01\n",
      "   5.08903742e-01 -3.17815900e-01  6.49582371e-02  5.79408169e-01\n",
      "  -1.07027754e-01  2.81879723e-01  2.12223738e-01  3.40128541e-01\n",
      "  -1.24587142e+00 -2.22344235e-01 -2.95936733e-01  2.41364554e-01\n",
      "  -2.58156598e-01  4.73059826e-02 -9.95852053e-03  1.61905944e-01\n",
      "   2.53077805e-01  6.40345812e-01  2.28748202e-01  2.13981166e-01\n",
      "   9.09286916e-01 -6.02205276e-01  4.21185732e-01 -2.38718480e-01\n",
      "   7.68581808e-01 -1.00393522e+00  3.01997006e-01  4.70188707e-01\n",
      "   3.19930196e-01 -5.78333378e-01  2.00729683e-01  2.27662638e-01\n",
      "  -1.10483676e-01  5.38155377e-01  4.36782777e-01 -1.75158903e-01\n",
      "  -1.11690772e+00  6.93241730e-02  1.71696767e-02  1.43703893e-01\n",
      "  -3.57155949e-02 -2.67826676e-01 -1.17234588e-01  5.79103976e-02\n",
      "  -2.14310929e-01  2.83654854e-02 -8.29345465e-01  3.08585316e-01\n",
      "   3.00283283e-01 -6.24611601e-02 -1.63141996e-01  4.11732495e-01\n",
      "   9.82594118e-02  3.61687005e-01 -9.32564586e-03  3.07581909e-02\n",
      "   7.01081753e-03 -1.15957111e-02 -4.62090969e-03 -3.77625585e-01\n",
      "  -2.29132786e-01 -3.81196141e-01 -7.42101818e-02  2.43510127e-01\n",
      "   3.69547099e-01  2.86817998e-01 -3.01568806e-01 -1.20727599e-01\n",
      "   2.07607687e-01  3.49657983e-01  9.32007372e-01  6.25702459e-03\n",
      "   6.80146515e-02  9.95979130e-01  4.72636402e-01 -7.45688617e-01\n",
      "  -1.91700315e+00 -2.43029505e-01 -5.89232385e-01 -1.04217663e-01\n",
      "   7.11048990e-02  7.00558722e-02  7.83213854e-01  4.06831093e-02\n",
      "   3.11891496e-01 -1.61306262e-01  3.61143202e-01 -6.61365747e-01\n",
      "  -4.53671277e-01 -3.71128619e-01 -3.24259460e-01 -7.24104643e-01\n",
      "   6.26288772e-01  3.72278690e-01  2.98029542e-01  3.28990161e-01\n",
      "   5.42921782e-01 -2.47404531e-01 -4.01903212e-01  1.57580122e-01\n",
      "  -1.57878429e-01  6.00147367e-01  4.42092538e-01  6.43648207e-01\n",
      "   2.55671144e-01 -1.50141820e-01  6.31747901e-01  8.11035156e-01\n",
      "   4.80711013e-01  8.28566074e-01  5.71691871e-01  4.13331449e-01\n",
      "   6.00519061e-01  5.48049286e-02 -7.25544244e-02 -3.01654994e-01\n",
      "  -4.21502680e-01 -2.56592870e-01 -3.22145045e-01  1.55641124e-01\n",
      "   7.60166883e-01 -1.10323519e-01 -1.73440516e-01 -1.59948051e-01\n",
      "   1.78717017e-01 -2.42862418e-01 -2.19001919e-01 -2.97920033e-02\n",
      "  -1.24918014e-01 -2.93215305e-01  2.45948404e-01  1.86527893e-01\n",
      "   1.54658878e+00  4.70663667e-01 -3.91976655e-01  9.32387590e-01\n",
      "  -7.79494941e-02 -9.99187455e-02 -1.98017478e-01  1.55311435e-01\n",
      "  -6.97934687e-01  6.84955567e-02 -2.65631557e-01 -1.82014093e-01\n",
      "   2.50943303e-01 -3.17425370e-01  4.53272223e-01  1.35905772e-01\n",
      "  -1.64754540e-01  2.68400222e-01 -2.55606115e-01 -1.15338564e-01\n",
      "  -7.29171038e-01  4.52326596e-01  5.89480639e-01 -4.59712029e-01\n",
      "   5.76209962e-01 -2.07220167e-02  1.48432374e-01 -2.40896881e-01\n",
      "  -2.88055956e-01 -7.61983767e-02  5.19333363e-01  6.86934054e-01\n",
      "   9.71992791e-01  3.30600262e-01 -1.51675045e-01  2.18459800e-01\n",
      "  -3.26706097e-02  9.30989906e-02  7.86158964e-02  3.10267478e-01\n",
      "  -1.08238474e-01  3.16912115e-01 -2.10655004e-01 -3.43456328e-01\n",
      "  -3.80241543e-01  3.00274074e-01 -5.86472601e-02 -5.23804605e-01\n",
      "  -1.60485774e-01  2.76986063e-02 -3.47171247e-01  4.39465016e-01\n",
      "   1.01832926e-01 -2.22424865e-01  5.54855391e-02 -1.59853905e-01\n",
      "  -4.03899610e-01 -1.23572379e-01 -5.09926826e-02 -8.95634651e-01\n",
      "   1.82007011e-02 -3.59395117e-01 -6.09333754e-01 -1.00458935e-02\n",
      "  -8.54860425e-01 -1.06521755e-01  7.81089067e-02  3.57347548e-01\n",
      "   4.44702953e-01  5.88686168e-02  2.92067349e-01 -6.24833286e-01\n",
      "  -2.79752791e-01 -9.97662425e-01 -1.46035224e-01 -2.75883764e-01\n",
      "   3.36676955e-01 -4.67054844e-01 -6.48395538e-01 -4.09809709e-01\n",
      "  -1.60747752e-01 -2.44734257e-01 -2.64243513e-01  5.62467426e-03\n",
      "   3.98928881e-01  2.47745186e-01  3.77315879e-01  2.93251202e-02\n",
      "   4.30133343e-01 -1.95681229e-01  7.48462230e-03  7.34736323e-02\n",
      "  -4.70481753e-01 -3.94629240e-01 -2.01644674e-02  3.05745274e-01\n",
      "  -4.43612248e-01 -4.18670885e-02 -8.12223107e-02 -5.05297422e-01\n",
      "   2.39195168e-01 -3.52398485e-01 -7.44368196e-01  3.30101579e-01\n",
      "  -1.68585479e-01  2.58205920e-01  1.65911660e-01  5.85577309e-01\n",
      "  -2.59040594e-01 -1.40700221e-01 -6.01411581e-01  7.96336770e-01\n",
      "   4.08484697e-01 -9.11857933e-02  1.08652189e-01 -1.21910840e-01\n",
      "   1.70348912e-01  5.89802563e-01 -8.64455327e-02 -2.23791488e-02\n",
      "  -5.35052538e-01 -1.05963312e-02  2.01046154e-01  5.57326227e-02\n",
      "   4.15074289e-01 -1.17445320e-01 -2.17601970e-01  2.95261085e-01\n",
      "  -6.40112042e-01 -1.26456261e+00 -5.65489471e-01 -3.94895941e-01\n",
      "  -5.69690287e-01 -9.34423357e-02 -6.26636386e-01 -4.29563493e-01\n",
      "  -7.13925898e-01 -3.94340642e-02 -4.65754867e-02  5.62559674e-03\n",
      "   2.45102167e-01 -1.47874370e-01  6.94124401e-03 -1.23434342e-01\n",
      "  -9.68733072e-01  2.25509107e-01 -7.02964421e-03  9.56642777e-02\n",
      "   9.85332608e-01  2.89259940e-01 -6.23666883e-01  4.23989221e-02\n",
      "  -1.30793065e-01  5.29444218e-03 -6.34555161e-01 -6.29072130e-01\n",
      "   2.08436832e-01  2.00631514e-01 -4.36674505e-01  1.10894091e-01\n",
      "  -1.09464034e-01 -9.96189177e-01 -1.13040194e-01  1.98916301e-01\n",
      "  -4.38163966e-01 -4.14197206e-01 -7.17565656e-01  2.97256023e-01\n",
      "   1.98631197e-01 -2.65550800e-03  2.67137587e-03  1.67545307e+00\n",
      "  -9.21595693e-02 -2.19245255e-02 -8.99805576e-02  2.38816828e-01\n",
      "  -3.24619591e-01 -8.37627053e-01 -1.07053362e-01 -3.12253326e-01\n",
      "  -6.45367742e-01 -1.51895553e-01  4.19944525e-01 -2.03886688e-01\n",
      "  -4.24875915e-02  7.01075912e-01  6.32063746e-01 -6.77734613e-01\n",
      "  -3.89624760e-03 -2.68287659e-01 -7.67435208e-02 -5.42067364e-02\n",
      "  -4.15078849e-01 -2.04213876e-02  4.25560594e-01  5.33863194e-02\n",
      "  -5.41662753e-01  3.04807350e-03 -1.35830238e-01  3.60549271e-01\n",
      "   2.40438968e-01 -3.37284625e-01 -5.37172258e-01  7.28873014e-02\n",
      "   2.45686069e-01  2.91004479e-01 -5.78897774e-01 -2.24989951e-01\n",
      "   4.74697441e-01  5.18058896e-01  4.94676471e-01  3.64543229e-01\n",
      "  -6.42664135e-01 -9.94842291e-01 -1.20312132e-01 -1.87917110e-02\n",
      "   1.34157375e-01  4.19233739e-01 -7.06016198e-02  3.01319122e-01\n",
      "   1.71860307e-01  1.96854159e-01 -5.67802072e-01  3.68316144e-01\n",
      "   2.78702099e-02 -1.10469675e+00 -1.59303665e-01  3.72493684e-01\n",
      "   4.40371782e-03 -3.56599599e-01  1.13608241e-01  1.90484300e-01\n",
      "   3.11769933e-01  2.37362355e-01 -9.63888243e-02 -1.11421144e+00\n",
      "  -1.92188039e-01 -1.77449748e-01 -2.01519012e-01 -5.73237896e-01\n",
      "   1.10160522e-01  1.58179373e-01  1.97370186e-01  5.47438085e-01\n",
      "  -1.74867511e-01 -9.35316145e-01  6.97023451e-01  4.80560094e-01\n",
      "  -1.22593179e-01 -4.24777657e-01  3.34913611e-01 -2.00776726e-01\n",
      "   3.91776890e-01  7.07376242e-01  2.83669055e-01  7.77225137e-01\n",
      "  -5.70929289e-01  3.27053219e-02  8.52335542e-02 -2.35217512e-01\n",
      "   1.14070572e-01  7.51418710e-01  4.51635122e-01 -8.32879186e-01\n",
      "  -2.88371556e-02  6.33641779e-01 -1.11225292e-01 -7.08267748e-01\n",
      "  -3.19597900e-01 -5.83316624e-01  2.59672940e-01  1.73288912e-01\n",
      "   7.61407912e-02 -1.32766867e+00 -1.32126367e+00  5.08496165e-01\n",
      "  -8.53022873e-01  2.58434340e-02  4.32578743e-01  9.39858019e-01\n",
      "   4.39851493e-01  4.35512036e-01 -5.06290972e-01 -1.59511387e-01\n",
      "   2.43927196e-01  2.56445026e+00  4.63133603e-01 -1.07642567e+00\n",
      "   7.38940418e-01  2.66427100e-01  6.00919962e-01 -6.87056720e-01\n",
      "  -1.35133576e+00 -1.16358125e+00 -2.29101956e-01  2.84674197e-01\n",
      "  -4.73121524e-01  4.12585884e-01  3.29452008e-02 -1.24997959e-01\n",
      "   8.84700119e-02  3.10759246e-01 -1.70777142e-01  4.48129386e-01\n",
      "   1.17049396e-01 -3.89040530e-01 -4.15206194e-01  2.65662283e-01\n",
      "  -4.96612132e-01 -2.09134057e-01  5.91791153e-01  2.46134907e-01\n",
      "  -5.40212095e-01  3.81145000e-01 -6.09050989e-01 -6.00727439e-01\n",
      "   6.37773722e-02  7.28767887e-02  1.14716873e-01  4.01827484e-01\n",
      "  -1.26379758e-01 -1.24819838e-01 -3.28616798e-02 -4.02929693e-01\n",
      "   1.16394155e-01  3.70674610e-01 -4.05473232e-01  2.13302583e-01\n",
      "  -5.11676073e-01  3.83991897e-01  5.15823007e-01  4.08508778e-01\n",
      "  -3.27389747e-01  7.69654989e-01  9.87005457e-02  4.94709849e-01\n",
      "   2.25896433e-01  3.69106948e-01  5.19592285e-01 -4.89323556e-01\n",
      "  -4.67815250e-02 -3.65461975e-01 -3.65155935e-01 -1.96281970e-02\n",
      "  -2.95736462e-01  1.09302968e-01  8.35478604e-02  1.47447497e-01\n",
      "   4.25231382e-02  5.30431807e-01  4.81173635e-01 -6.37808204e-01\n",
      "   3.92664671e-02  2.95571804e-01  1.56983316e-01  4.14703377e-02\n",
      "   8.81251916e-02 -5.38415968e-01 -2.11324230e-01 -2.11042359e-01\n",
      "   8.54777634e-01  2.41319895e-01  1.60600945e-01 -5.66721439e-01\n",
      "  -7.27542400e-01 -4.83769551e-02 -2.98531353e-02 -6.30434632e-01\n",
      "  -6.49743341e-03  4.42675948e-01  1.72820210e-01 -2.57762432e-01\n",
      "  -5.69442511e-01  4.46060598e-01  1.59345210e-01 -2.88814813e-01\n",
      "   7.02794433e-01  1.73609391e-01  3.09867322e-01  1.27180472e-01\n",
      "   2.25426555e-01  1.97239611e-02 -6.68848217e-01 -6.09499030e-02\n",
      "   6.77945167e-02  8.60574394e-02  2.75948524e-01  1.87716067e-01\n",
      "   2.26585418e-01  1.80782184e-01  1.27679095e-01  1.33520043e+00\n",
      "   4.59177464e-01  2.33904257e-01 -6.77829087e-01  2.90896356e-01\n",
      "  -2.73779929e-01 -1.57689959e-01 -3.83909196e-01  1.81873441e-01\n",
      "   2.34820217e-01  4.75863844e-01 -1.34415537e-01 -2.27069080e-01\n",
      "   3.59819025e-01 -3.15121293e-01 -3.72116655e-01  8.95286500e-01\n",
      "   1.32883176e-01  1.99947610e-01 -2.22896948e-01  6.10621274e-02\n",
      "  -6.37284458e-01  1.20777555e-01  1.21930629e-01 -1.20980382e-01\n",
      "  -1.95272148e-01 -2.13656396e-01  4.50536132e-01 -5.68991303e-01\n",
      "   5.09823382e-01 -1.77539185e-01 -1.51677862e-01  2.75614500e-01\n",
      "  -4.30136442e-01 -2.63609737e-01 -3.70238364e-01 -3.83576155e-02\n",
      "  -2.62272954e-01  1.21550366e-01  3.08616668e-01  1.36790156e-01\n",
      "   2.08183587e-01 -9.27404910e-02  5.36060274e-01  2.26525277e-01\n",
      "   5.24484932e-01 -4.24091041e-01 -7.94555098e-02 -6.93106651e-02\n",
      "   3.75419974e-01  1.53273791e-02 -6.92357793e-02 -6.07932329e-01\n",
      "  -9.49065536e-02 -1.64288193e-01  6.33210182e-01  3.90407443e-01\n",
      "  -2.22790867e-01  5.33875704e-01 -3.15049469e-01  3.85716259e-02\n",
      "  -2.61553794e-01 -5.12481570e-01 -4.38878685e-01 -1.39485431e+00\n",
      "  -1.11795092e+00 -1.92306980e-01  1.92496017e-01 -1.73850715e-01\n",
      "   8.04431736e-03  7.26681411e-01 -1.37132168e-01 -3.07125896e-01\n",
      "   4.73417521e-01  1.07589237e-01  6.33948445e-01 -3.44647646e-01\n",
      "  -3.89535055e-02 -8.88757408e-01 -2.87028790e-01 -5.91608524e-01\n",
      "  -4.33431044e-02  8.22874308e-02  1.37748748e-01 -1.16077530e+00\n",
      "  -1.37350589e-01 -7.79029608e-01  3.81602019e-01  1.51957065e-01\n",
      "  -1.10304572e-01  3.63939762e-01 -3.19710553e-01 -2.70201802e-01\n",
      "  -3.15291584e-02 -4.46252793e-01 -7.77652860e-01  4.51920629e-01\n",
      "  -9.20308053e-01  3.14883590e-01  3.53447974e-01 -2.33929902e-01\n",
      "  -5.48295230e-02 -2.55250603e-01 -1.61223248e-01 -4.67053086e-01\n",
      "   1.99260980e-01 -1.31502539e-01 -4.93242085e-01  3.52399170e-01\n",
      "   7.87947834e-01  2.10250661e-01 -8.10701177e-02 -5.12817055e-02\n",
      "   2.77112275e-01 -1.26549256e+00  6.53632283e-02  1.20260969e-01\n",
      "   2.19430059e-01 -1.09593995e-01 -1.42989933e-01 -3.40956181e-01\n",
      "   1.96168453e-01  6.96411878e-02  4.88490760e-02 -3.34428132e-01\n",
      "  -1.34417355e-01  3.18873882e-01  3.40571761e-01  8.39852765e-02\n",
      "  -4.76027220e-01  9.07048464e-01 -1.31366581e-01  4.92303520e-02\n",
      "  -6.13704324e-03  4.89629507e-02 -1.54578865e-01  5.48274636e-01\n",
      "  -2.94913679e-01  7.85884142e-01  3.32230926e-01  3.21883142e-01\n",
      "   6.03510201e-01 -1.65144831e-01 -1.51387244e-01  2.80996174e-01\n",
      "  -8.29365969e-01  2.76239336e-01  6.37438893e-02 -1.01165485e+00\n",
      "   1.28633216e-01 -6.54950738e-03 -2.93329120e-01  1.47094131e-01\n",
      "   3.06367576e-01 -8.62885937e-02  3.87425095e-01  7.53211200e-01\n",
      "   1.07958995e-01 -6.07750565e-02  6.60111606e-01  2.97920913e-01\n",
      "   1.53852805e-01  4.07428801e-01 -1.17726862e-01  1.14196670e+00\n",
      "   3.30420911e-01 -4.65557218e-01 -4.76120293e-01  6.20998740e-01\n",
      "   4.20741737e-01  4.39153731e-01  1.46619350e-01  2.40784883e-03\n",
      "  -7.02720731e-02 -1.65891588e-01  4.76259515e-02  3.57625127e-01\n",
      "  -1.97357997e-01  9.84717011e-02  4.12251830e-01 -1.55565119e+00\n",
      "  -1.64170787e-01  6.45015717e-01 -4.12731826e-01 -2.23437369e-01\n",
      "  -5.15833378e-01  1.23069189e-01 -2.31530786e-01  2.58992612e-01\n",
      "  -4.05785769e-01 -4.24278438e-01 -2.43202180e-01  1.79202452e-01\n",
      "   3.26839238e-01  5.89075759e-02  2.05851883e-01  1.97656214e-01\n",
      "  -1.60781577e-01 -1.24524742e-01  3.17990273e-01 -1.02998567e+00\n",
      "   3.80144715e-01 -6.20664716e-01  4.78374571e-01  9.41820621e-01\n",
      "   2.94901013e-01  5.38707674e-02  9.83590335e-02 -4.14059311e-02\n",
      "  -8.09719801e-01 -3.93839031e-01  1.54476881e-01  1.53318495e-01\n",
      "   3.91292840e-01  7.10125089e-01 -3.56255472e-01  1.64571166e-01]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "#Cargamos el modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = AutoModel.from_pretrained(checkpoint_path)\n",
    "\n",
    "#Creamos el texto y tokenizamos\n",
    "texto = \"I want a chocolate ice cream\"\n",
    "inputs = tokenizer(texto, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "#Mandamos los token al modelo para que lo procese y genere embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "#Ahora seleccionamos los embeddings del token cls que están contextualizados\n",
    "embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "#Imprimimos los embeddings\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora generamos embeddings del texto no del token CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings y tokens guardados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = AutoModel.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Asegúrate de que el modelo esté en modo de evaluación\n",
    "model.eval()\n",
    "\n",
    "# Verificar si hay GPU disponible y usarla\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Texto de entrada para generar embeddings\n",
    "texto = \"Jacqueline Hyde starts like any other normal day for telemarketing individual Jackie Hyde (co-producer Gabriella Hall) until her boss (Robert Donovan) fires her for taking personal calls at work, however it's not all bad news as the call she took was from a lawyer informing her that her Grandfather (Malcolm Bennett) has recently died & that he left her his mansion & fortune (why doesn't stuff like that ever happen to me? Sigh). Very excited Jackie heads on over there & makes herself right at home, while looking for the thermostat late one night Jackie stumbles upon a secret room where her Grandfather stashes the bright red formula that he invented that allows whoever drinks it to change their appearance. Being a bit on the porky side Jackie finally settles on the glamorous Jacqueline (Blythe Metz), however Jackie's better looking alter-ego starts to take control...<br /><br />Written, co-produced & directed by Rolfe Kanefsky I thought Jacqueline Hyde was complete total & utter crap from start to finish & it's as simple & straight forward as that. According to the opening credits Jacqueline Hyde was 'inspired' by the classic Robert Louis Stevenson novel 'The Strange Case of Dr. Jekyll and Mr. Hyde', frankly if Mr. Stevenson could see what was being done to his story here he'd turn in his grave. For a start I think Jacqueline Hyde was/is intended to be a horror film, the IMDb certainly lists it as such but there isn't any horror in it at all apart from just how bad it is. I would say that Jacqueline Hyde is more a soft-core porno than anything else & extremely tame with it, why sit down & watch this softer than soft porno crap when you can watch you proper hard-core stuff that actually delivers the goods? Why, that's the question I ask here. It's not even good porn either, besides being far too soft it's dull, boring & the not-worth-mentioning sex scenes are few & far between. The most intelligent aspect of this film is the title which would have been quite clever if not for the fact that another film used the Jacqueline Hyde (1998) title during the last century & judging by the IMDb's plot summary it sounds a hell of a lot better than this piece of rubbish. This is one of those films you have to watch yourself to see just how bad it is but just hope that you never get the opportunity.<br /><br />Director Kanefsky was obviously working on a low budget but that's not an excuse these days, shot on a digital camcorder the film looks cheap & the few instances of CGI look like they came from a Nintendo Gameboy, the final 'shocking' twist has probably the worst morph effect I've ever seen & is pretty good for a laugh as is the scene when Jackie's breasts grow via more terrible CGI. That's another thing, the film takes itself far too seriously. The subject matter sucks, is far too predictable & makes for a poor film but maybe if the dialogue had been intentionally funny with some dirty porn talk the film might have been more fun to watch, alas it isn't so it isn't. Forget about any decent horror, violence or gore as there isn't any apart from a surprisingly bloodless decapitation at the end.<br /><br />Technically Jacqueline Hyde is home made film type stuff, the photography is of the flat hand held point-&-shoot variety, the music, production design & special effects are of a suitably low standard to match the script. The acting was awful, seriously this is bad.<br /><br />Jacqueline Hyde in my opinion a load of crap, there is not one positive thing about this turgid film that I can think of. Any proper film lover will have an almost impossible time trying to find any redeeming value in this crap, definitely one to avoid.\"\n",
    "\n",
    "#Limpiamos el texto con Gensim\n",
    "texto_limpio = ' '.join(simple_preprocess(texto, deacc=True))\n",
    "\n",
    "# Tokenización del texto\n",
    "inputs = tokenizer(texto_limpio, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "# Mover los tensores a la GPU si está disponible\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# Desactivar el cálculo de gradientes\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtener los embeddings de todos los tokens de la última capa\n",
    "embeddings = outputs.last_hidden_state[0].cpu().numpy()\n",
    "\n",
    "# Obtener las IDs tokenizadas\n",
    "input_ids = inputs['input_ids'][0].cpu().tolist()\n",
    "\n",
    "# Filtrar los tokens y embeddings (omitimos el token [CLS] y [PAD])\n",
    "tokens = []\n",
    "embeddings_list = []\n",
    "\n",
    "for i, (token_id, token_embedding) in enumerate(zip(input_ids[1:], embeddings[1:]), start=1):\n",
    "    if token_id == [tokenizer.pad_token_id, tokenizer.sep_token_id]:\n",
    "        break  #  Detenerse si se encuentra el token de padding o [SEP]\n",
    "    token = tokenizer.decode([token_id])\n",
    "    tokens.append(token)\n",
    "    embeddings_list.append(token_embedding)\n",
    "\n",
    "# Guardar los embeddings en un archivo TSV\n",
    "with open('text_embeddings.tsv', 'w', encoding='utf-8') as f_embeddings:\n",
    "    for embedding in embeddings_list:\n",
    "        # Convierte cada embedding en una línea de texto separado por tabulaciones\n",
    "        embedding_str = '\\t'.join(map(str, embedding))\n",
    "        f_embeddings.write(embedding_str + '\\n')\n",
    "\n",
    "# Guardar los tokens en un archivo TSV\n",
    "with open('text_labels.tsv', 'w', encoding='utf-8') as f_labels:\n",
    "    for token in tokens:\n",
    "        f_labels.write(token + '\\n')\n",
    "\n",
    "print(\"Embeddings y tokens guardados exitosamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
